{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dadcaaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0   247547  2015-06-28 03:00:34  2015-08-09 03:57:29        0.546375   \n",
      "1   220737  2015-01-28 14:21:11  2015-02-11 20:28:28       -1.193991   \n",
      "2   390400  2015-03-19 20:49:09  2015-04-11 23:41:23        0.383216   \n",
      "3    69592  2015-02-24 06:11:57  2015-05-23 16:40:14        0.981467   \n",
      "4   174987  2015-07-07 12:58:11  2015-11-03 04:04:30        0.763921   \n",
      "\n",
      "   device_id  source  sex       age    ip_address  class  ip_address_int  \\\n",
      "0      26918       2    0 -0.360216  1.677886e+07      0       -1.730003   \n",
      "1      40340       2    0  0.102304  1.684205e+07      0       -1.729900   \n",
      "2      30757       0    1 -0.475846  1.684366e+07      0       -1.729897   \n",
      "3      53074       1    0 -0.360216  1.693873e+07      0       -1.729742   \n",
      "4      61816       2    0  0.449195  1.697198e+07      0       -1.729687   \n",
      "\n",
      "   purchase_time_diff  hour_of_day  day_of_week  transaction_frequency  \\\n",
      "0                 0.0    -1.230236     1.483770                    0.0   \n",
      "1                 0.0     1.230149    -0.510683                    0.0   \n",
      "2                 0.0     1.664335     0.985157                    0.0   \n",
      "3                 0.0     0.651235     0.985157                    0.0   \n",
      "4                 0.0    -1.085508    -1.009297                    0.0   \n",
      "\n",
      "   transaction_velocity  browser_FireFox  browser_IE  browser_Opera  \\\n",
      "0                   0.0            False       False          False   \n",
      "1                   0.0            False       False          False   \n",
      "2                   0.0            False        True          False   \n",
      "3                   0.0            False       False          False   \n",
      "4                   0.0            False       False          False   \n",
      "\n",
      "   browser_Safari  \n",
      "0            True  \n",
      "1           False  \n",
      "2           False  \n",
      "3           False  \n",
      "4           False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from the specified path\n",
    "df = pd.read_csv(\"../data/processed/Fraud__Data.csv\")\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'device_id' column\n",
    "df['device_id'] = label_encoder.fit_transform(df['device_id'])\n",
    "\n",
    "# Optionally, save the updated DataFrame back to the CSV file\n",
    "df.to_csv(\"../data/processed/Fraud__Data_Encoded.csv\", index=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48ef7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  purchase_value  device_id  source  sex       age    ip_address  \\\n",
      "0   247547        0.546375      26918       2    0 -0.360216  1.677886e+07   \n",
      "1   220737       -1.193991      40340       2    0  0.102304  1.684205e+07   \n",
      "2   390400        0.383216      30757       0    1 -0.475846  1.684366e+07   \n",
      "3    69592        0.981467      53074       1    0 -0.360216  1.693873e+07   \n",
      "4   174987        0.763921      61816       2    0  0.449195  1.697198e+07   \n",
      "\n",
      "   class  ip_address_int  purchase_time_diff  ...  purchase_hour  \\\n",
      "0      0       -1.730003                 0.0  ...              3   \n",
      "1      0       -1.729900                 0.0  ...             20   \n",
      "2      0       -1.729897                 0.0  ...             23   \n",
      "3      0       -1.729742                 0.0  ...             16   \n",
      "4      0       -1.729687                 0.0  ...              4   \n",
      "\n",
      "   purchase_dayofweek  purchase_month  purchase_year  signup_hour  \\\n",
      "0                   6               8           2015            3   \n",
      "1                   2               2           2015           14   \n",
      "2                   5               4           2015           20   \n",
      "3                   5               5           2015            6   \n",
      "4                   1              11           2015           12   \n",
      "\n",
      "   signup_dayofweek  signup_month  signup_year  purchase_time_numeric  \\\n",
      "0                 6             6         2015             1439092649   \n",
      "1                 2             1         2015             1423686508   \n",
      "2                 3             3         2015             1428795683   \n",
      "3                 1             2         2015             1432399214   \n",
      "4                 1             7         2015             1446523470   \n",
      "\n",
      "   signup_time_numeric  \n",
      "0           1435460434  \n",
      "1           1422454871  \n",
      "2           1426798149  \n",
      "3           1424758317  \n",
      "4           1436273891  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/processed/Fraud__Data.csv\")\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'device_id' column\n",
    "df['device_id'] = label_encoder.fit_transform(df['device_id'])\n",
    "\n",
    "# Convert 'purchase_time' and 'signup_time' to datetime\n",
    "df['purchase_time'] = pd.to_datetime(df['purchase_time'])\n",
    "df['signup_time'] = pd.to_datetime(df['signup_time'])\n",
    "\n",
    "# Extract useful time-related features for 'purchase_time'\n",
    "df['purchase_hour'] = df['purchase_time'].dt.hour  # Hour of the day\n",
    "df['purchase_dayofweek'] = df['purchase_time'].dt.dayofweek  # Day of the week (0 = Monday, 6 = Sunday)\n",
    "df['purchase_month'] = df['purchase_time'].dt.month  # Month (1 to 12)\n",
    "df['purchase_year'] = df['purchase_time'].dt.year  # Year\n",
    "\n",
    "# Extract useful time-related features for 'signup_time'\n",
    "df['signup_hour'] = df['signup_time'].dt.hour  # Hour of the day\n",
    "df['signup_dayofweek'] = df['signup_time'].dt.dayofweek  # Day of the week (0 = Monday, 6 = Sunday)\n",
    "df['signup_month'] = df['signup_time'].dt.month  # Month (1 to 12)\n",
    "df['signup_year'] = df['signup_time'].dt.year  # Year\n",
    "\n",
    "# Optionally, convert 'purchase_time' and 'signup_time' to Unix timestamp\n",
    "df['purchase_time_numeric'] = df['purchase_time'].astype('int64') // 10**9  # Convert to seconds\n",
    "df['signup_time_numeric'] = df['signup_time'].astype('int64') // 10**9  # Convert to seconds\n",
    "\n",
    "# Drop the original 'purchase_time' and 'signup_time' columns\n",
    "df = df.drop(columns=['purchase_time', 'signup_time'])\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "df.to_csv(\"../data/processed/Fraud__Data_Encoded.csv\", index=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cbbf3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "fraud_data = pd.read_csv(\"../data/processed/Fraud__Data.csv\")\n",
    "creditcard_data = pd.read_csv(\"../data/processed/creditcard.csv\")\n",
    "\n",
    "# Separate features and target for both datasets\n",
    "X_fraud = fraud_data.drop(columns=['class'])\n",
    "y_fraud = fraud_data['class']\n",
    "\n",
    "X_credit = creditcard_data.drop(columns=['Class'])\n",
    "y_credit = creditcard_data['Class']\n",
    "\n",
    "# Combine both datasets (ensure the features match)\n",
    "X_combined = pd.concat([X_fraud, X_credit], axis=0)\n",
    "y_combined = pd.concat([y_fraud, y_credit], axis=0)\n",
    "\n",
    "# Train-Test Split for combined data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_combined, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8e84a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "user_id                  0\n",
      "purchase_value           0\n",
      "device_id                0\n",
      "source                   0\n",
      "sex                      0\n",
      "age                      0\n",
      "ip_address               0\n",
      "class                    0\n",
      "ip_address_int           0\n",
      "purchase_time_diff       0\n",
      "hour_of_day              0\n",
      "day_of_week              0\n",
      "transaction_frequency    0\n",
      "transaction_velocity     0\n",
      "browser_FireFox          0\n",
      "browser_IE               0\n",
      "browser_Opera            0\n",
      "browser_Safari           0\n",
      "purchase_hour            0\n",
      "purchase_dayofweek       0\n",
      "purchase_month           0\n",
      "purchase_year            0\n",
      "signup_hour              0\n",
      "signup_dayofweek         0\n",
      "signup_month             0\n",
      "signup_year              0\n",
      "purchase_time_numeric    0\n",
      "signup_time_numeric      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after replacement:\n",
      "user_id                  0\n",
      "purchase_value           0\n",
      "device_id                0\n",
      "source                   0\n",
      "sex                      0\n",
      "age                      0\n",
      "ip_address               0\n",
      "class                    0\n",
      "ip_address_int           0\n",
      "purchase_time_diff       0\n",
      "hour_of_day              0\n",
      "day_of_week              0\n",
      "transaction_frequency    0\n",
      "transaction_velocity     0\n",
      "browser_FireFox          0\n",
      "browser_IE               0\n",
      "browser_Opera            0\n",
      "browser_Safari           0\n",
      "purchase_hour            0\n",
      "purchase_dayofweek       0\n",
      "purchase_month           0\n",
      "purchase_year            0\n",
      "signup_hour              0\n",
      "signup_dayofweek         0\n",
      "signup_month             0\n",
      "signup_year              0\n",
      "purchase_time_numeric    0\n",
      "signup_time_numeric      0\n",
      "dtype: int64\n",
      "\n",
      "Updated DataFrame:\n",
      "   user_id  purchase_value  device_id  source  sex       age    ip_address  \\\n",
      "0   247547        0.546375      26918       2    0 -0.360216  1.677886e+07   \n",
      "1   220737       -1.193991      40340       2    0  0.102304  1.684205e+07   \n",
      "2   390400        0.383216      30757       0    1 -0.475846  1.684366e+07   \n",
      "3    69592        0.981467      53074       1    0 -0.360216  1.693873e+07   \n",
      "4   174987        0.763921      61816       2    0  0.449195  1.697198e+07   \n",
      "\n",
      "   class  ip_address_int  purchase_time_diff  ...  purchase_hour  \\\n",
      "0      0       -1.730003                 0.0  ...              3   \n",
      "1      0       -1.729900                 0.0  ...             20   \n",
      "2      0       -1.729897                 0.0  ...             23   \n",
      "3      0       -1.729742                 0.0  ...             16   \n",
      "4      0       -1.729687                 0.0  ...              4   \n",
      "\n",
      "   purchase_dayofweek  purchase_month  purchase_year  signup_hour  \\\n",
      "0                   6               8           2015            3   \n",
      "1                   2               2           2015           14   \n",
      "2                   5               4           2015           20   \n",
      "3                   5               5           2015            6   \n",
      "4                   1              11           2015           12   \n",
      "\n",
      "   signup_dayofweek  signup_month  signup_year  purchase_time_numeric  \\\n",
      "0                 6             6         2015             1439092649   \n",
      "1                 2             1         2015             1423686508   \n",
      "2                 3             3         2015             1428795683   \n",
      "3                 1             2         2015             1432399214   \n",
      "4                 1             7         2015             1446523470   \n",
      "\n",
      "   signup_time_numeric  \n",
      "0           1435460434  \n",
      "1           1422454871  \n",
      "2           1426798149  \n",
      "3           1424758317  \n",
      "4           1436273891  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/processed/Fraud__Data.csv\")\n",
    "\n",
    "# Check for missing values (NaN or None) in each column\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Option 1: Replace missing values with the column's mean (for numerical columns)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Option 2: Replace missing values with the mode (most frequent value) for categorical columns\n",
    "# For categorical columns, you may want to replace NaNs with the mode\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Check again if there are any missing values after replacement\n",
    "print(\"\\nMissing values after replacement:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Optionally, save the updated DataFrame\n",
    "df.to_csv(\"../data/processed/Fraud__Data_NoNulls.csv\", index=False)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"\\nUpdated DataFrame:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f81fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline with imputation, scaling, and logistic regression\n",
    "logreg_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Impute missing values using the mean\n",
    "    StandardScaler(),  # Scale the data\n",
    "    LogisticRegression(random_state=42, max_iter=500)  # Increase max_iter if needed\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3950f7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved to: ../models/trained_models/logistic_regression_model.joblib\n",
      "Accuracy: 0.988109376964887\n",
      "Confusion Matrix:\n",
      "[[70042    58]\n",
      " [  793   676]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     70100\n",
      "           1       0.92      0.46      0.61      1469\n",
      "\n",
      "    accuracy                           0.99     71569\n",
      "   macro avg       0.95      0.73      0.80     71569\n",
      "weighted avg       0.99      0.99      0.99     71569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Create a pipeline with imputation, scaling, and logistic regression\n",
    "logreg_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Impute missing values using the mean\n",
    "    StandardScaler(),  # Scale the data\n",
    "    LogisticRegression(random_state=42, max_iter=500)  # Increase max_iter if needed\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Step 2: Save the trained Logistic Regression model\n",
    "model_path = \"../models/trained_models/logistic_regression_model.joblib\"\n",
    "joblib.dump(logreg_model, model_path)\n",
    "\n",
    "print(f\"Logistic Regression model saved to: {model_path}\")\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5047d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved to: ../models/trained_models/Decision_Tree_model.joblib\n",
      "Accuracy: 0.9794324358311559\n",
      "Confusion Matrix:\n",
      "[[69252   848]\n",
      " [  624   845]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     70100\n",
      "           1       0.50      0.58      0.53      1469\n",
      "\n",
      "    accuracy                           0.98     71569\n",
      "   macro avg       0.75      0.78      0.76     71569\n",
      "weighted avg       0.98      0.98      0.98     71569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Create a pipeline with imputation, scaling, and decision tree classifier\n",
    "dt_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Impute missing values using the mean\n",
    "    StandardScaler(),  # Scale the data\n",
    "    DecisionTreeClassifier(random_state=42)  # Decision Tree Classifier\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Step 2: Save the trained Logistic Regression model\n",
    "model_path = \"../models/trained_models/Decision_Tree_model.joblib\"\n",
    "joblib.dump(logreg_model, model_path)\n",
    "\n",
    "print(f\"Logistic Regression model saved to: {model_path}\")\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "05cceecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved to: ../models/trained_models/Random_Forest_model.joblib\n",
      "Accuracy: 0.9905685422459445\n",
      "Confusion Matrix:\n",
      "[[70075    25]\n",
      " [  650   819]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     70100\n",
      "           1       0.97      0.56      0.71      1469\n",
      "\n",
      "    accuracy                           0.99     71569\n",
      "   macro avg       0.98      0.78      0.85     71569\n",
      "weighted avg       0.99      0.99      0.99     71569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Create a pipeline with imputation, scaling, and random forest classifier\n",
    "rf_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Impute missing values using the mean\n",
    "    StandardScaler(),  # Scale the data\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest Classifier\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Step 2: Save the trained Random_Forest model\n",
    "model_path = \"../models/trained_models/Random_Forest_model.joblib\"\n",
    "joblib.dump(logreg_model, model_path)\n",
    "\n",
    "print(f\"Logistic Regression model saved to: {model_path}\")\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0deb5972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Gradient_Boosting saved to: ../models/trained_models/Gradient_Boosting_model.joblib\n",
      "Accuracy: 0.9903449817658483\n",
      "Confusion Matrix:\n",
      "[[70065    35]\n",
      " [  656   813]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     70100\n",
      "           1       0.96      0.55      0.70      1469\n",
      "\n",
      "    accuracy                           0.99     71569\n",
      "   macro avg       0.97      0.78      0.85     71569\n",
      "weighted avg       0.99      0.99      0.99     71569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Create a pipeline with imputation, scaling, and gradient boosting classifier\n",
    "gb_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Impute missing values using the mean\n",
    "    StandardScaler(),  # Scale the data\n",
    "    GradientBoostingClassifier(n_estimators=100, random_state=42)  # Gradient Boosting Classifier\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Step 2: Save the trained Gradient_Boosting model\n",
    "model_path = \"../models/trained_models/Gradient_Boosting_model.joblib\"\n",
    "joblib.dump(logreg_model, model_path)\n",
    "\n",
    "print(f\"Logistic Gradient_Boosting saved to: {model_path}\")\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2daac7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic MLP Classifier saved to: ../models/trained_models/MLP_Classifier_model.joblib\n",
      "Accuracy: 0.9904288169458844\n",
      "Confusion Matrix:\n",
      "[[70069    31]\n",
      " [  654   815]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     70100\n",
      "           1       0.96      0.55      0.70      1469\n",
      "\n",
      "    accuracy                           0.99     71569\n",
      "   macro avg       0.98      0.78      0.85     71569\n",
      "weighted avg       0.99      0.99      0.99     71569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "\n",
    "# Create a pipeline with imputation, scaling, and multi-layer perceptron classifier\n",
    "mlp_model = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),  # Impute missing values using the mean\n",
    "    StandardScaler(),  # Scale the data\n",
    "    MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)  # MLP Classifier with 1 hidden layer of 100 neurons\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = mlp_model.predict(X_test)\n",
    "\n",
    "# Step 2: Save the trained MLP Classifier model\n",
    "model_path = \"../models/trained_models/MLP_Classifier_model.joblib\"\n",
    "joblib.dump(logreg_model, model_path)\n",
    "\n",
    "print(f\"Logistic MLP Classifier saved to: {model_path}\")\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c822dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
